\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{color}
\usepackage[space]{grffile}
\usepackage{array}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{chngpage}


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage[font=small,labelfont=bf,tableposition=top]{caption}

%\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}
%\makeatletter 
%\let\@oldmaketitle\@maketitle% Store \@maketitle
%\renewcommand{\@maketitle}
%{
%{\@oldmaketitle% Update \@maketitle to insert... 
%\centerline{
%\begin{tabular}{c}
%\includegraphics[width=\textwidth]{figs/fig1_sffsr.pdf}
%\end{tabular}
%%\begin{figure*}
%%\includegraphics{figs/sr10.png}
%%\end{figure*}
%}
%\bigskip}% ... an image 
%\vspace{-15pt}
%\captionof{figure}{\label{fig:fig1}
%(Top) Our results using a single network for all scale factors. Super-resolved images over all scales are clean and sharp. (Bottom)  Results of Dong et al.  \cite{Dong2014} ($\times$3 model used for all scales). Result images are not visually pleasing. To handle multiple scales, existing methods require multiple networks. }
%\vspace{15pt}
%}
%\makeatother


%%%%%%%%% TITLE
\title{Deeply-Recursive Convolutional Network for Image Super-Resolution}

\author{First Author\\
	Institution1\\
	Institution1 address\\
	{\tt\small firstauthor@i1.org}
	% For a paper whose authors are all at the same institution,
	% omit the following lines up until the closing ``}''.
	% Additional authors and addresses can be added with ``\and'',
	% just like the second author.
	% To save space, use either the email address or home page, not both
	\and
	Second Author\\
	Institution2\\
	First line of institution2 address\\
	{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

% TODO if net was trained very deep, it can be used in a shallow way for low-end PCs or mobiles


%%%%%%%%% ABSTRACT
\begin{abstract}
We propose an image super-resolution method (SR) using a deeply-recursive convolutional network (RCN). A RCN super-resolves images with a single recursive layer. Our network utilizes very large context without dimensionality reduction such as pooling. As our method reuse the same parameters, our network is much smaller than the same network architecture without parameter sharing. Optimizing a deep RCN, however, does not easily converge with a standard gradient descent method. To resolve the issue, we propose two extensions: recursive-supervision and skip-connection. We successfully learned a SR method with 25 recursions. Our method outperforms previous methods by a large margin.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
For image super-resolution (SR), receptive field determines the amount of contextual information that can be exploited to infer missing high-frequency components. For example, if there exists a pattern with smoothed edges contained in a receptive field, it is plausible the pattern is recognized and edges are sharpened. As SR is an ill-posed inverse problem, collecting and analyzing more neighbor pixels can possibly give more clues on what have been lost by downsampling. We experimentally check the statement by widening receptive field while network capacity fixed in this work.  

Deep convolutional networks (DCN) succeeding in various computer vision tasks typically use very large receptive fields  (224x224 common in ImageNet classification \cite{krizhevsky2012imagenet, simonyan2015very}). Receptive field is typically enlarged by either a convolution (conv.) or a pooling (pool.).  Both approaches have drawbacks: a conv. layer introduces more parameters and a pool. layer typically discards some pixel-wise information. 

%While recent studies have shown that unpooling recovers some information about the original image (CITE), Alexey paper has shown inverting CNN cannot reconstruct the image perfectly.

For image restoration problems such as super-resolution and denoising, image details are very important. Therefore, most deep-learning approaches for such problems do not use pooling. This implies enlarging receptive field of a typical convolutional network can only be achieved by introducing a new conv. layer.

Increasing depth by adding a new weight layer basically introduces more parameters. Two problems can arise. First, overfitting is highly likely. More data are now required. Second, model becomes too huge to be stored and retrieved.

 
%[TODO List some efforts put into enlarging patch sizes in traditional SR literature in this paragraph.]

%[TODO Mention deep-learning SR methods.] A famous deep-learning SR method SRCNN \cite{Dong2014}, however, use only three layers resulting in a receptive field of 13 by 13. To utilize contextual information spread over very large image regions, many conv. layers need to be stacked in standard CNN settings. If $3 \times 3$ filters are stacked, stacking 10 layers result in $21 \times 21$ receptive field. This is still very small compared to typical image sizes.


To resolve the issues, we propose a SR method using a deeply-recursive convolutional network (RCN). A RCN repeatedly applies the same convolutional layer as many times as desired. The number of parameters are kept small while widening receptive fields. Our receptive field reaches 41 by 41 and this is very large compared to other deep-learning based image restoration methods. 

Overfitting is largely alleviated in our model. As the same parameters are used again and again, the network is effectively regularized. Moreover, our model can be economically stored and distributed online.   

While a recursive approach has good properties, learning long-range dependencies between pixels with a single weight layer is very difficult. A RCN optimized with the widely-used stochastic gradient descent method does not easily converge. This is due to exploding/vanishing gradients. 

We propose two approaches to ease the difficulty of training. First, all recursions are simultaneously supervised. Feature maps after each recursion are used to reconstruct the target HR image. Reconstruction method (layers dedicated to reconstruction) is the same for all recursions. As each recursion leads to a HR prediction, we combine all predictions resulting from different levels of recursions to deliver the final prediction.  
   
The second proposal is to use skip-connections from input to reconstruction layers. In SR, low-resolution image (input) and high-resolution image (output) share the same information to a large extent. Exact copy of input, however, is likely to be attenuated during many forward passes. We explicitly connect the input to the layers for output reconstruction. This is particularly effective when input and output are highly correlated.


\textbf{Contributions} In summary, we propose a image super-resolution method deeply recursive in nature. It utilizes very large context compared to previous SR methods with only a single recursive layer. We improve the simple recursive network in two ways: recursive-supervision and skip-connection. Our method demonstrates the state-of-the-art performance in common benchmarks for SR.

\section{Related Work}
\subsection{Single-Image Super-Resolution}

Our deep RCN is applied to the problem of generating a high-resolution (HR) image given a low-resolution (LR) image: single-image SR \cite{Irani1991, freeman2000learning,glasner2009super}. Many SR methods have been proposed in the computer vision community. Early methods use very fast interpolations but they give poor results. Some of more powerful methods utilize statistical image priors \cite{sun2008image,Kim2010} or internal patch recurrence \cite{glasner2009super, Huang-CVPR-2015}.

Recently, sophisticated learning methods are widely used to model a mapping from LR to HR patches. Many methods have paid attention to find better regression functions from LR to HR images. This is achieved with various techniques: neighbor embedding \cite{chang2004super,bevilacqua2012}, sparse coding \cite{yang2010image,zeyde2012single,Timofte2013,Timofte}, convolutional neural network (CNN) \cite{Dong2014} and random forest \cite{schulter2015fast}.

Among several recent learning-based successes,  convolutional neural network (SRCNN) \cite{Dong2014} demonstrated the feasibility of an end-to-end approach to SR. One possibility to improve SRCNN is to simply stack more weight layers as many times as possible. However, this significantly increases more parameters and requires more data unless effective regularization is used.  

In this work, we seek to find a function that effectively model pixel dependencies as long as possible. Our network model recursively widens receptive field without increasing the model capacity and dependency between very distant pixels (compared to existing methods) is utilized. The concept of recursion itself actually is a kind of regularization technique applied to very deep SRCNN. 

\subsection{Recursive Neural Network in Computer Vision}

%
%Recursive convolution is first proposed in \cite{Eigen2014}. (Socher et al. [45]?? Eigen et. al) to understand deep architectures. They used up to three 

Recursive neural networks, suitable for temporal and sequential data, have seen limited use on algorithms operating on a single static image.   Socher et al.  \cite{socher2012convolutional} used a convolutional network in a separate stage to first learn features on RGB-Depth data, prior to hierarchical merging. In these models the input dimension is twice that of the output and recursive convolutions are applied only two times. In Eigen et. al \cite{Eigen2014}, recursive layers have the same input and output dimension, but recursive convolutions resulted in worse performances than a single convolution due to overfitting. 

To overcome overfitting, Liang and Hu \cite{Liang_2015_CVPR} uses a recurrent layer that takes feed-forward inputs into all unfolded layers. They show that performance increases up to three convolutions. Their network structure, designed for object recognition, is the same as the existing CNN architectures.

While \cite{Eigen2014} and \cite{Liang_2015_CVPR} simply modify existing architectures to apply convolutions up to three times, our network is completely different from multi-layer approaches. To our knowledge, we demonstrate for the first time that a single recursive layer mainly solves a non-trivial vision task (SR). 

In addition, we demonstrate that very deep recursions significantly boost the performance. We apply the same convolution up to 25 times (previous maximum is three). It is an interesting future direction to see if a single-recursive-layer approach can work for other tasks.  

\section{Proposed Method}
In this section, our proposed method is explained. We first explain a basic model using recursive convolutions and discuss its limitations. Then we propose an improved model and the training procedure for it.

\begin{figure*}[t]
	\includegraphics[width=\textwidth]{figs/f1}
	\caption {Architecture of our basic model. It consists of three parts: embedding network, inference network and reconstruction network. Inference network has a recursive layer and its unfolded version is in Figure \ref{fig:inference_network}}
	\label{fig:overview}
\end{figure*}

\subsection{Basic Model}

Our network, outlined in Figure \ref{fig:overview}, consists of three sub-networks: embedding, inference and reconstruction networks. \textbf{Embedding net} is used to represent the given image as feature maps ready for inference. Next, \textbf{inference net} solves the task. Once inference is done, final feature maps in inference net are fed into \textbf{reconstruction net} to generate the output image.

We now look into each sub-network. \textbf{Embedding net} takes the input image (grayscale or RGB) and represent it as a set of feature maps. Intermediate representation used to pass information to inference net largely depends on how inference net internally represent its feature maps in their hidden layers. Learning this representation is done end-to-end altogether with learning other sub-networks.


\textbf{Inference net} is the main component that solves the task, super-resolution. Analyzing a large image region is done by a single recursive layer. Each recursion applies the same convolution followed by a rectified linear unit. With convolution filters larger than $1\times 1$, receptive field is widened every recursion.   

\begin{figure}
	\includegraphics[width=0.5\textwidth]{figs/f2}
	\caption {Unfolding inference network. \textbf{Left}: A recursive layer \textbf{Right}: Unfolded structure. The same filters W are applied to feature maps recursively. Our model can utilize very large context without adding new weight parameters. }
	\label{fig:inference_network}
\end{figure}

While feature maps from the final application of the recursive layer represent the high-resolution image, transformation of them (multi-channel) back into the original image space (1 or 3-channel) is necessary. This is done by \textbf{reconstruction net}.  

We have a single hidden layer for each sub-net. Only the layer for inference net is recursive. Other sub-nets are vastly similar to standard mutilayer perceptrons (MLP) with a single hidden layer. For MLP, full connection of $F$ neurons is equivalent to a convolution with $1\times 1\times F \times F$. In our sub-nets, we use $3\times 3\times F \times F$ filters. For embedding net, we use $3\times 3$ filters because image gradients are more informative than the raw intensities for super-resolution. For inference net, $3\times 3$ convolutions imply that hidden states are passed to adjacent pixels only. Reconstruction net also considers direct neighbors for transformation.  

\textbf{Mathematical Formulation} Now we give the mathematical formulation of our model. The network takes an interpolated input image (to the desired size) as input ${\bf x}$ and predicts the target image ${\bf y}$ as in SRCNN \cite{Dong2014}. Our goal is to learn a model $f$ that predicts values $\mathbf{\hat{y}}=f(\mathbf{x})$.  

 Let $f_1, f_2, f_3$ denote sub-net functions: embedding, inference and reconstruction, respectively. Our model is the composition of three functions: $f({\bf y}) = f_3(f_2 (f_1({\bf x}))).$
 
 Embedding net $f_1({\bf x})$ takes the input vector ${\bf x}$ and computes the matrix output $H_0$, which is an input to the inference net $f_2$. Hidden layer values are denoted by $H_{-1}$. The formula for embedding net is as follows:
  \begin{align}
        H_{-1} &= max(0, W_{-1}*{\bf x} + b_{-1})\\
        H_0 &= max(0, W_{0}*H_{-1} + b_0)\\
        f_1({\bf x}) &= H_0,
    \end{align}
where the operator $*$ denotes a convolution and $max(0,\cdot)$ corresponds to a ReLU. Weight and bias matrices are $W_{-1},W_0$ and $b_{-1},b_0$.

Inference net $f_2$ takes the input matrix $H_0$ and computes the matrix output $H_{D}$. Here, we use the same weight and bias matrices $W$ and $b$ for all operations.  Let $g$ denote the function modeled by a single recursion of the recursive layer: $g(H)=max(0,W*H+b)$. The recurrence relation is  
\begin{equation}
 H_d = g(H_{d-1}) = max(0,W*H_{d-1}+b),
\end{equation}
for $d = 1, ..., D$. 

Inference net $f_2$ is equivalent to the composition of the same elementary functions $g$: 
\begin{equation}
f_2(H) = g \circ g \circ \cdots \circ g(H) =  g^{D}(H).
\end{equation}

Reconstruction net $f_3$ takes the input hidden state $H_D$ and outputs the target image (high-resolution). Roughly speaking, reconstruction net is the inverse operation of embedding net. The formula is as follows:
\begin{align}
	H_{D+1} &= max(0, W_{D+1}*H_D + b_{D+1})\\
	\hat{{\bf y}} &= max(0, W_{D+2}*H_{D+1} + b_{D+2})\\
	f_3(H) &= \hat{{\bf y}}.
\end{align}

\textbf{Model Properties} Now we have all components for our model $f({\bf y}) = f_3(f_2 (f_1({\bf x})))$. The recursive model has pros and cons. One good thing is that the model requires no new parameters for additional recursions. That means widening receptive field to utilize larger image context can be done with the network capacity fixed. 

While the recursive model is simple and powerful, we find training a deeply-recursive network very difficult. Also, the maximum number of recursions successful in previous methods is three \cite{Liang_2015_CVPR}.  Among many reasons, two severe problems are \textit{vanishing} and \textit{exploding gradients} \cite{bengio1994learning, pascanu2013difficulty}.  

\textit{exploding gradients} refer to the large increase in the norm
of the gradient during training. Such events are due to
the multiplicative nature of chained gradients. Long term components can grow exponentially and this problem occurs only for deep recursions.


The
\textit{vanishing gradients} problem refers to the opposite behavior. Long term components approach exponentially
fast to zero vector. Due to it, learn relation between distant pixels is very hard.

In addition to gradient problems, there exists an issue with finding the optimal number of recursions. In theory, very large recursions are always good since the network sees more image region and we hope it learn to keep the important information and discard the unnecessary.  But in practice, a fixed-sized vector carrying all context information until the end of all recursions might lack capacity if recursions are too deep. 

To resolve the gradient and optimal recursion issues, we propose an advanced model.

\subsection{Advanced Model} 
To overcome the weaknesses of the simple model, we propose two extensions: recursive-supervision and skip-connection. Deep-supervision for convolutional network is first proposed in Lee et al  \cite{lee2014deeply}. Their method simultaneously minimizes classification error while improving the directness and transparency of the hidden layer learning process.

\textbf{Recursive-Supervision} We recursively-supervise all convolutions in order to alleviate the effect of vanishing/exploding gradients. There are two significant differences from our recursive-supervision to the original deep-supervision. In \cite{lee2014deeply}, they associate one classifier for each hidden layer. For each additional layer, new classifier has to be introduced and new parameters thereby. If this approach is used, our modified network looks as in Figure \ref{fig:recursive_supervision} (b). We need $D$ different reconstruction networks. This is against our original purpose of using recursive networks: not introducing new parameters while stacking more layers. 

As we have assumed that the same representation can be used again and again during convolutions in inference net, much better regularization is to use the same recon. net for all recursions. Our recon. net now outputs $D$ predictions and all predictions are simultaneously supervised during training (Figure \ref{fig:recursive_supervision}). 

The second difference of our method to the original deep-supervision is that we use all $D$ intermediate predictions to compute the final output. All predictions are averaged during testing. The optimal weights are automatically learned during training. In contrast,  \cite{lee2014deeply} discard all intermediate classifiers during testing. 

Our recursive-supervision naturally eases the difficulty of training recursive networks. Backpropagation goes through small number of layers if supervising signal goes directly from loss layer to early recursion. Summing all gradients backpropagated from different prediction losses give smoothing effect. The effect of vanishing/exploding gradients along one backpropagation path is alleviated.

Moreover, the importance of picking the optimal number of recursions is reduced as our supervision enables utilizing predictions from all intermediate layers. If recursions are too deep for the given task, we expect the weight for the intermediate predictions high. By looking at weights of predictions, we can figure out the marginal gain from additional recursions. 

\textbf{Skip-connection} Now we describe our second extension: skip-connection. We find input and output images are highly correlated. Carrying most if not all of input values until the end of the network is necessary. Due to gradient problems, learning that input and output are mostly similar is very difficult with deep recursions.  

Adding layer skips \cite{bishop2006pattern} is successfully used for a semantic segmentation network \cite{long2014fully}. We employ a similar idea: input image is directly fed into the recon. net. The skip-connection has two advantages. First, Network capacity to store the input signal during recursions is saved. Second, the exact copy of input signal can be used during target prediction. 

Our skip-connection is simple yet very effective. In super-resolution, LR and HR images are vastly similar. In most regions, differences are zero and only small number of locations have non-zero values. Modeling image details is often used in super-resolution methods \cite{Timofte2013, Timofte, bevilacqua2012,bevilacqua2013super}, but we demonstrate that this domain-specific knowledge can significantly improve an general end-to-end learning method like deep-learning, especially if the net is deeply-recursive.
As illustrated in Figure \ref{fig:recursive_supervision} (a), the inference network now learns a mapping to very sparse predictions and the required capacity to solve the task gets significantly reduced. 

\textbf{Mathematical Formulation} We revisit the mathematical formulation of our model with two extensions. In the simple model, the prediction is $\hat{{\bf y}} = f_3(H_D)$, where $f_3$ and $H_D$ denote the recon. net and the final hidden state of the inference net, respectively. With skip-connection,  
\begin{equation}
\hat{{\bf y}} = f_3'({\bf x}, f_2(f_1({\bf x}))).
\end{equation}


%\hat{{\bf y}} = {\bf x} + f_3(f_2(f_1({\bf x}))) = {\bf x} + f_3(g^{(D)}(f_1({\bf 
Each intermediate prediction under recursive-supervision is 
\begin{equation}
\hat{{\bf y}}_{d} = f_3'({\bf x}, g^{(d)}(f_1({\bf x}))).
\end{equation}
Recon. net with skip-connection $f_3({\bf x}, H_d)$ can take various functional forms. For example, input can be concatenated to the feature maps $H_d$. As the input is an interpolated input image (roughly speaking, $\hat{\bf y} \approx {\bf x}$), we find $f_3'({\bf x}, H_d) = {\bf x} + f_3(H_d)$ is enough for our purpose. 


Now, the final output is the weighted average of all intermediate predictions:
\begin{equation}
\hat{{\bf y}}^{(final)} = \sum_{d=1}^{D} w_d \cdot \hat{{\bf y}}_d.
\end{equation}
where $w_d$ denotes the weights of predictions reconstructed from each intermediate hidden state during recursion.
%
%\begin{figure}
%\begin{center}
%	\includegraphics[width=0.4\textwidth]{figs/f4}
%	\caption{An illustration of using residual learning. By learning residual parts only, we can get two advantages. First, the problem space is shrinked to be learned more easily. Second, An useful information of input can be conserved intact to the end.}
%\end{center}
%\end{figure}

\begin{figure*}
\begin{center}
	\includegraphics[width=\textwidth]{figs/f3}
	\caption{(a): Our final (advanced) model with recursive-supervision and skip-connection. The reconstruction network is shared for recursive predictions. We use all predictions from the intermediate recursion to get the final output. (b): Applying deep-supervision to our simple model \cite{lee2014deeply}. Unlike in (a), they use different reconstruction networks for recursions and more parameters are used.  (c) Expanded structure of (a) without parameter sharing (no recursion). The number of weight parameters is proportional to the depth squared.  }
\label{fig:recursive_supervision}
\end{center}
\end{figure*}

\subsection{Training}

\textbf{Objective} We now describe the training objective to minimize to find optimal parameters of our model. Given training dataset $\{{\bf x}^{(i)},{\bf y}^{(i)}\}{}_{i=1}^{N}$, our goal is to find the best model $f$ that predicts values $\mathbf{\hat{y}}=f(\mathbf{x})$.

In the least-squares regression setting, typical in SR, the mean squared error $\frac{1}{2}||\mathbf{y}-f(\mathbf{x})||^{2}$
averaged over training set is minimized. This favors high Peak Signal-to-Noise
Ratio (PSNR), a widely-used evaluation criteria. 

With recursive-supervision, we have $D+1$ objectives: supervising $D$ outputs from recursions and the final output. For intermediate outputs, we have the loss function 
\begin{equation}
l_1(\theta) = \sum_{d=1}^D \sum_{i=1}^N \frac{1}{2}||{\bf y}^{(i)} -  \hat{\bf y}_d^{(i)} ||^{2},
\end{equation}
where $\theta$ denotes the parameter set and $\hat{\bf y}_d^{(i)}$ is the output from the $d$-th recursion. For the final output, we have 
\begin{equation}
l_2(\theta) = \sum_{i=1}^N \frac{1}{2}||{\bf y}^{(i)} -  \sum_{d=1}^D  w_d \cdot \hat{\bf y}_d^{(i)} ||^{2}
\end{equation}

Now we give the final loss function $L(\theta)$. The training is regularized by weight decay ($L_2$ penalty multiplied by $\beta$). 
\begin{equation}
L(\theta)  =\alpha  l_1(\theta) + (1 - \alpha) l_2(\theta) + \beta ||\theta||^2,
\end{equation}
where $\alpha$ denotes the importance of the companion objective on the intermediate outputs and $\beta$ denotes the multiplier of weight decay.   Setting $\alpha$ high makes the training procedure stable as early recursions easily converge. As training progresses, $\alpha$ decays to boost the performance of the final output. 

Training is carried out by optimizing the regression objective using mini-batch gradient descent based on back-propagation (LeCun et al. \cite{lecun1998gradient}). We implement our model using the \textit{MatConvNet}\footnote{\url{ http://www.vlfeat.org/matconvnet/}} package \cite{arXiv:1412.4564}. 

\section{Experimental Results}
TODO replace. Simonyan and Zisserman \cite{simonyan2015very} have demonstrated the effectiveness of stacking small filters many times and making a network (very) deep. We similarly use small filters ($3\times3$) for all conv. layers. 


TODO replace. We set the momentum parameter to 0.9 and weight decay to 0.0001.

TODO initialization

%
%Training deep models often fail to converge. He et al. \cite{he2015delving} uses a theoretically sound initialization method which helps very deep models converge when training from scratch and they succeed in training 30 weight layers. They, however, report no benefit from training extremely deep models for their problem. In our work, adding layers are beneficial in general. For large scale factors, deep models exploiting contextual information spread in very large field are dominant. 
%
%Training the multi-scale model is straightforward. Training datasets for several specified scales are combined into one big dataset. We demonstrate in the next section that a model learned with this works under multiple scales.  
%
%\textcolor{red}{Data preparation is similar to SRCNN \cite{Dong2014} with some differences. Input patch size is equal to the size of receptive field and images are divided into sub-images with no overlap. 64 sub-images constitue a mini-batch, where sub-images from different scales can be in the same batch.}
%
%
%

In this section, we evaluate the performance of our method on several datasets. We first describe datasets used for training and testing our method. Next, parameters necessary for training are given. 

After outlining our experimental setup, we compare our method with several state-of-the-art SISR methods. 

\subsection{Datasets for Training and Testing}
\textbf{Training dataset} For training, we use images proposed in Yang et al. \cite{yang2010image}, which is used by other learning-based methods \cite{Timofte,Timofte2013,zeyde2012single}. 

\textbf{Test dataset} For benchmark, we use four datasets. Datasets `Set5' \cite{bevilacqua2012} and `Set14' \cite{zeyde2012single} are often used for benchmark in other works \cite{Timofte,Timofte2013,Dong2014}. Dataset `Urban100', a dataset of urban images recently provided by Huang et al. \cite{Huang-CVPR-2015} is very interesting as it contains many challenging images failed by many of existing methods. Finally, dataset `B100', natural images in the Berkeley Segmentation Dataset, used in Timofte et al. \cite{Timofte} and Yang and Yang \cite{Yang2013} for benchmark, is also employed. 
\subsection{Training Parameters}
We provide parameters used to train our final model. We use a network of depth 20. Training uses batches of size 64. Momentum and weight decay parameters are set to 0.9 and $0.0001$, respectively. [TODO: explain in last part of Section 3?]

For weight initialization, we use the method described in He et al. \cite{he2015delving} for the embedding net and reconstruction net. This is a theoretically sound procedure for networks utilizing rectified linear units (ReLu). And for inference net, we initialize filters $W$ as identity. [TODO : Is it right?]

We train all experiments over? epochs (9960 iterations with batch size 64). Learning rate was initially set to 0.0005 and then decreased by a factor of 10 every 20 epochs. In total, the learning rate was decreased 3 times, and the learning
is stopped after ? epochs. Training takes roughly 4 hours on GPU Titan X. 


\subsection{Comparisons with State-of-the-Art Methods}
We provide quantitative and qualitative comparisons. For benchmark, we use published code for A+ \cite{Timofte}, SRCNN \cite{dong2014image}, RFL \cite{schulter2015fast} and  Huang et al. \cite{Huang-CVPR-2015}. We apply bicubic interpolation to color components of an image and sophisticated models to luminance components as in  other methods \cite{chang2004super}, \cite{glasner2009super}, \cite{zeyde2012single}. This is because human is much more sensitive to visually observe details in intensity than in color. 

As  some other compared methods like A+ \cite{Timofte} and  RFL \cite{schulter2015fast} don't predict image boundary, they need to crop pixels near image boundary. For our method, this procedure is unnecessary as our network super-resolves an entire image. For fair comparison, however, we also crop pixels to the same amount. \footnotemark{}

\footnotetext{PSNRs are slightly different from the original paper as they use different evaluation framework.}


\begin{table*}
\begin{center}
\setlength{\tabcolsep}{2pt}
\footnotesize
\begin{tabular}{ | c | c | c | c | c | c | c | c | }
\hline
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Scale} & Bicubic & A+ & SRCNN & RFL & SelfEx & RCN (Ours)\\
 & & PSNR/SSIM & PSNR/SSIM & PSNR/SSIM & PSNR/SSIM & PSNR/SSIM & PSNR/SSIM\\
\hline
\hline
\multirow{3}{*}{Set5} & $\times$2 & 33.66/0.9299 & 36.54/{\color{blue}0.9544} & {\color{blue}36.66}/0.9542 & 36.54/0.9537 & 36.49/0.9537 & {\color{red}37.35}/{\color{red}0.9575}\\
 & $\times$3 & 30.39/0.8682 & 32.58/0.9088 & {\color{blue}32.75}/0.9090 & 32.43/0.9057 & 32.58/{\color{blue}0.9093} & {\color{red}33.62}/{\color{red}0.9211}\\
 & $\times$4 & 28.42/0.8104 & 30.28/0.8603 & {\color{blue}30.48}/{\color{blue}0.8628} & 30.14/0.8548 & 30.31/0.8619 & {\color{red}31.27}/{\color{red}0.8804}\\
\hline
\hline
\multirow{3}{*}{Set14} & $\times$2 & 30.24/0.8688 & 32.28/0.9056 & {\color{blue}32.42}/{\color{blue}0.9063} & 32.26/0.9040 & 32.22/0.9034 & {\color{red}32.89}/{\color{red}0.9098}\\
 & $\times$3 & 27.55/0.7742 & 29.13/0.8188 & {\color{blue}29.28}/{\color{blue}0.8209} & 29.05/0.8164 & 29.16/0.8196 & {\color{red}29.69}/{\color{red}0.8300}\\
 & $\times$4 & 26.00/0.7027 & 27.32/0.7491 & {\color{blue}27.49}/0.7503 & 27.24/0.7451 & 27.40/{\color{blue}0.7518} & {\color{red}27.88}/{\color{red}0.7629}\\
\hline
\hline
\multirow{3}{*}{B100} & $\times$2 & 29.56/0.8431 & 31.21/0.8863 & {\color{blue}31.36}/{\color{blue}0.8879} & 31.16/0.8840 & 31.18/0.8855 & {\color{red}31.74}/{\color{red}0.8916}\\
 & $\times$3 & 27.21/0.7385 & 28.29/0.7835 & {\color{blue}28.41}/{\color{blue}0.7863} & 28.22/0.7806 & 28.29/0.7840 & {\color{red}28.74}/{\color{red}0.7955}\\
 & $\times$4 & 25.96/0.6675 & 26.82/0.7087 & {\color{blue}26.90}/0.7101 & 26.75/0.7054 & 26.84/{\color{blue}0.7106} & {\color{red}27.16}/{\color{red}0.7193}\\
\hline
\hline
\multirow{3}{*}{Urban100} & $\times$2 & 26.88/0.8403 & 29.20/0.8938 & 29.50/0.8946 & 29.11/0.8904 & {\color{blue}29.54}/{\color{blue}0.8967} & {\color{red}30.42}/{\color{red}0.9087}\\
 & $\times$3 & 24.46/0.7349 & 26.03/0.7973 & 26.24/0.7989 & 25.86/0.7900 & {\color{blue}26.44}/{\color{blue}0.8088} & {\color{red}27.01}/{\color{red}0.8245}\\
 & $\times$4 & 23.14/0.6577 & 24.32/0.7183 & 24.52/0.7221 & 24.19/0.7096 & {\color{blue}24.79}/{\color{blue}0.7374} & {\color{red}24.97}/{\color{red}0.7431}\\
\hline
\end{tabular}
\caption{Average PSNR/SSIM for scale factor $\times$2, $\times$3 and $\times$4 on datasets Set5, Set14, B100 and Urban100. {\color{red}Red color} indicates the best performance and {\color{blue}blue color} indicates the second best one.}
\end{center}
\end{table*}


In Table \ref{table_all}, we provide a summary of quantitative evaluation on several datasets. In addition to our model trained with 91 images, we provide another model trained with 291 images (91 + 200 natural images from Berkeley Segmentation Dataset \cite{Martin2001}).
%We also provide RCN+ and RCN-291+, where transformed images (combination of flips and 90 deg. rotations) are also used for training.
Our methods outperform all previous methods in these datasets. Moreover, our methods are very fast.

[TODO : describing qualitative results]
In Figures \ref{fig:c1}, \ref{fig:c2} and \ref{fig:c3} we compare our method with top-performing methods. In Figure \ref{fig:c1}, only our method perfectly reconstructs the line in the middle. Similarly, in Figures \ref{fig:c2} and \ref{fig:c3}, lines are clean and vivid in our method whereas they are severely blurred or distorted in other methods.

\begin{figure*}
\begin{adjustwidth}{0cm}{-0.5cm}
\begin{center}
\small
\setlength{\tabcolsep}{5pt}
\begin{tabular}{ c C{3.5cm}  C{3.5cm}  C{3.5cm}  }
\multirow{4}{*}{\graphicspath{{figs/fig1/}}\includegraphics[width=0.27\textwidth]{img096_GTbox.png}}
& \raisebox{-13.0ex} {\graphicspath{{figs/fig1/}}\includegraphics[width=0.2\textwidth]{img096_for_fig1_HR.png}}\vspace{0.3ex}
& \raisebox{-13.0ex} {\graphicspath{{figs/fig1/}}\includegraphics[width=0.2\textwidth]{img096_for_fig1_A+.png}}\vspace{0.3ex}
& \raisebox{-13.0ex} {\graphicspath{{figs/fig1/}}\includegraphics[width=0.2\textwidth]{img096_for_fig1_SRCNN.png}}\vspace{0.3ex}
\\
& Original(PSNR, SSIM)& A+ (23.99, 0.8320)& SRCNN (24.74, 0.8459)\\
& \raisebox{-13.0ex} {\graphicspath{{figs/fig1/}}\includegraphics[width=0.2\textwidth]{img096_for_fig1_RFL.png}}\vspace{0.3ex}
& \raisebox{-13.0ex} {\graphicspath{{figs/fig1/}}\includegraphics[width=0.2\textwidth]{img096_for_fig1_SelfEx.png}}\vspace{0.3ex}
& \raisebox{-13.0ex} {\graphicspath{{figs/fig1/}}\includegraphics[width=0.2\textwidth]{img096_for_fig1_RCN.png}}\vspace{0.3ex}
\\
& RFL (23.95, 0.8230)& SelfEx (25.46, 0.8635)& RCN (25.86, 0.8867)\\
\end{tabular}
\caption{Super-resolution results of ``img096"(Urban100) with scale factor $\times$3. Our result is visually pleasing.}
\end{center}
\end{adjustwidth}
\end{figure*}

\begin{figure*}
\begin{adjustwidth}{0.5cm}{0.5cm}
\begin{center}
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{  c  c  c  c  c  c  }
{\graphicspath{{figs/fig2/}}\includegraphics[width=0.15\textwidth]{img082_for_fig2_HR.png}}
& {\graphicspath{{figs/fig2/}}\includegraphics[width=0.15\textwidth]{img082_for_fig2_A+.png}}
& {\graphicspath{{figs/fig2/}}\includegraphics[width=0.15\textwidth]{img082_for_fig2_SRCNN.png}}
& {\graphicspath{{figs/fig2/}}\includegraphics[width=0.15\textwidth]{img082_for_fig2_RFL.png}}
& {\graphicspath{{figs/fig2/}}\includegraphics[width=0.15\textwidth]{img082_for_fig2_SelfEx.png}}
& {\graphicspath{{figs/fig2/}}\includegraphics[width=0.15\textwidth]{img082_for_fig2_RCN.png}}
\\
Original& A+& SRCNN& RFL& SelfEx& RCN\\
(PSNR, SSIM)& (32.49, 0.9425)& (32.59, 0.9409)& (32.35, 0.9389)& (33.21, 0.9453)& (34.46, 0.9553)\\
\end{tabular}
\caption{Super-resolution results of ``img082"(Urban100) with scale factor $\times$3. Our result is visually pleasing.}
\end{center}
\end{adjustwidth}
\end{figure*}

\begin{figure*}
\begin{adjustwidth}{0.5cm}{0.5cm}
\begin{center}
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{  c  c  c  c  c  c  }
{\graphicspath{{figs/fig2/}}\includegraphics[width=0.15\textwidth]{img099_for_fig2_HR.png}}
& {\graphicspath{{figs/fig2/}}\includegraphics[width=0.15\textwidth]{img099_for_fig2_A+.png}}
& {\graphicspath{{figs/fig2/}}\includegraphics[width=0.15\textwidth]{img099_for_fig2_SRCNN.png}}
& {\graphicspath{{figs/fig2/}}\includegraphics[width=0.15\textwidth]{img099_for_fig2_RFL.png}}
& {\graphicspath{{figs/fig2/}}\includegraphics[width=0.15\textwidth]{img099_for_fig2_SelfEx.png}}
& {\graphicspath{{figs/fig2/}}\includegraphics[width=0.15\textwidth]{img099_for_fig2_RCN.png}}
\\
Original& A+& SRCNN& RFL& SelfEx& RCN\\
(PSNR, SSIM)& (24.97, 0.7606)& (25.49, 0.7710)& (24.53, 0.7460)& (25.65, 0.7921)& (26.30, 0.8177)\\
\end{tabular}
\caption{Super-resolution results of ``img099"(Urban100) with scale factor $\times$3. Our result is visually pleasing.}
\end{center}
\end{adjustwidth}
\end{figure*}

%
%
\section{Conclusion}
In this work, we have presented a super-resolution method using a deeply-recursive convolutional network. Our network uses a fixed number of parameters while exploiting a large image context. To ease the difficulty of training the model, we use residual-learning and deep-supervision. We have demonstrated that our method outperforms the existing method by a large margin on benchmarked images. In the future, one can try more recursions in order to use full image-level information. We believe our approach is readily applicable to other image restoration problems such as denoising, compression artifact removal and deblurring.

{\small
	\bibliographystyle{ieee}
	\bibliography{RCN}
}
\end{document}

% Some texs that are not used but possibly needed someday
%\begin{figure*}[t]
%	\centering
%	\begin{subfigure}{0.33\textwidth}
%		\centering
%		\includegraphics[width=0.2\textwidth]{../../../../srv1/matconvnet/SRCNN/scripts/net_intermidiate/2-01.png} 		\caption{Test Scale Factor 2}
%		\label{fig:gull}
%	\end{subfigure}%
%	\hfill
%	\begin{subfigure}{0.33\textwidth}
%		\centering
%		\includegraphics[width=0.2\textwidth]{../../../../srv1/matconvnet/SRCNN/scripts/net_intermidiate/3-01.png} 		\caption{Test Scale Factor 2}
%		\label{fig:gull}
%	\end{subfigure}%
%	\hfill
%	\begin{subfigure}{0.33\textwidth}
%		\centering
%		\includegraphics[width=0.2\textwidth]{../../../../srv1/matconvnet/SRCNN/scripts/net_intermidiate/4-01.png} 		\caption{Test Scale Factor 2}
%		\label{fig:gull}
%	\end{subfigure}%
%	
%	\begin{subfigure}{0.33\textwidth}
%		\centering
%		\includegraphics[width=0.6\textwidth]{../../../../srv1/matconvnet/SRCNN/scripts/net_intermidiate/2-02.png} 		\caption{Test Scale Factor 2}
%		\label{fig:gull}
%	\end{subfigure}%
%	\hfill
%	\begin{subfigure}{0.33\textwidth}
%		\centering
%		\includegraphics[width=0.6\textwidth]{../../../../srv1/matconvnet/SRCNN/scripts/net_intermidiate/3-02.png} 		\caption{Test Scale Factor 2}
%		\label{fig:gull}
%	\end{subfigure}%
%	\hfill
%	\begin{subfigure}{0.33\textwidth}
%		\centering
%		\includegraphics[width=0.6\textwidth]{../../../../srv1/matconvnet/SRCNN/scripts/net_intermidiate/4-02.png} 		\caption{Test Scale Factor 2}
%		\label{fig:gull}
%	\end{subfigure}%
%	
%	\begin{subfigure}{0.33\textwidth}
%		\centering
%		\includegraphics[width=0.6\textwidth]{../../../../srv1/matconvnet/SRCNN/scripts/net_intermidiate/2-03.png} 		\caption{Test Scale Factor 2}
%		\label{fig:gull}
%	\end{subfigure}%
%	\hfill
%	\begin{subfigure}{0.33\textwidth}
%		\centering
%		\includegraphics[width=0.6\textwidth]{../../../../srv1/matconvnet/SRCNN/scripts/net_intermidiate/3-03.png} 		\caption{Test Scale Factor 2}
%		\label{fig:gull}
%	\end{subfigure}%
%	\hfill
%	\begin{subfigure}{0.33\textwidth}
%		\centering
%		\includegraphics[width=0.6\textwidth]{../../../../srv1/matconvnet/SRCNN/scripts/net_intermidiate/4-03.png} 		\caption{Test Scale Factor 2}
%		\label{fig:gull}
%	\end{subfigure}%
%	
%	\begin{subfigure}{0.33\textwidth}
%		\centering
%		\includegraphics[width=0.6\textwidth]{../../../../srv1/matconvnet/SRCNN/scripts/net_intermidiate/2-38.png} 		\caption{Test Scale Factor 2}
%		\label{fig:gull}
%	\end{subfigure}%
%	\hfill
%	\begin{subfigure}{0.33\textwidth}
%		\centering
%		\includegraphics[width=0.6\textwidth]{../../../../srv1/matconvnet/SRCNN/scripts/net_intermidiate/3-38.png} 		\caption{Test Scale Factor 2}
%		\label{fig:gull}
%	\end{subfigure}%
%	\hfill
%	\begin{subfigure}{0.33\textwidth}
%		\centering
%		\includegraphics[width=0.6\textwidth]{../../../../srv1/matconvnet/SRCNN/scripts/net_intermidiate/4-38.png} 		\caption{Test Scale Factor 2}
%		\label{fig:gull}
%	\end{subfigure}%
%		
%	\begin{subfigure}{0.33\textwidth}
%		\centering
%		\includegraphics[width=0.6\textwidth]{../../../../srv1/matconvnet/SRCNN/scripts/net_intermidiate/2-39.png} 		\caption{Test Scale Factor 2}
%		\label{fig:gull}
%	\end{subfigure}%
%	\hfill
%	\begin{subfigure}{0.33\textwidth}
%		\centering
%		\includegraphics[width=0.6\textwidth]{../../../../srv1/matconvnet/SRCNN/scripts/net_intermidiate/3-39.png} 		\caption{Test Scale Factor 2}
%		\label{fig:gull}
%	\end{subfigure}%
%	\hfill
%	\begin{subfigure}{0.33\textwidth}
%		\centering
%		\includegraphics[width=0.6\textwidth]{../../../../srv1/matconvnet/SRCNN/scripts/net_intermidiate/4-39.png} 		\caption{Test Scale Factor 2}
%		\label{fig:gull}
%	\end{subfigure}%
%		
%	%(or a blank line to force the subfigure onto a new line)
%	\begin{subfigure}{0.33\textwidth}
%		\centering
%		\includegraphics[width=0.5\textwidth]{../../../../srv1/matconvnet/SRCNN/scripts/net_intermidiate/2-40.png} 		\caption{Test Scale Factor 2}
%		\label{fig:gull}
%	\end{subfigure}%
%	\hfill
%	\begin{subfigure}{0.33\textwidth}
%		\centering
%		\includegraphics[width=0.5\textwidth]{../../../../srv1/matconvnet/SRCNN/scripts/net_intermidiate/3-40.png} 		\caption{Test Scale Factor 2}
%		\label{fig:gull}
%	\end{subfigure}%
%	\hfill
%	\begin{subfigure}{0.33\textwidth}
%		\centering
%		\includegraphics[width=0.5\textwidth]{../../../../srv1/matconvnet/SRCNN/scripts/net_intermidiate/4-40.png} 		\caption{Test Scale Factor 2}
%		\label{fig:gull}
%	\end{subfigure}%
%	
%\end{figure*}
%
%\begin{table*}[t]
%	\small
%	\centering
%	\begin{tabular}
%		{|c|c|c|c|c|c|c|}
%		\hline
%		Set5        & Scale     & {Bicubic} & {SRCNN (2)}        & {SRCNN (3)}        & {SRCNN (4)}        & {Ours}            \\
%		\hline
%		baby        & $\times$2 & 37.07     & \color{blue} 38.30 & 33.55              & 30.12              & \color{red} 38.55 \\
%		bird        & $\times$2 & 36.81     & \color{blue} 40.64 & 32.25              & 28.08              & \color{red} 41.51 \\
%		butterfly   & $\times$2 & 27.43     & \color{blue} 32.20 & 24.88              & 21.28              & \color{red} 33.31 \\
%		head        & $\times$2 & 34.86     & \color{blue} 35.64 & 32.78              & 30.52              & \color{red} 35.77 \\
%		woman       & $\times$2 & 32.14     & \color{blue} 34.94 & 28.04              & 24.47              & \color{red} 35.52 \\
%		\hline
%		\hline
%		\bf Average & $\times$2 & 33.66     & \color{blue} 36.34 & 30.30              & 26.89              & \color{red} 36.93 \\
%		\hline
%		\hline
%		baby        & $\times$3 & 33.91     & 34.12              & \color{blue} 35.01 & 32.78              & \color{red} 35.27 \\
%		bird        & $\times$3 & 32.58     & 32.99              & \color{blue} 34.91 & 31.51              & \color{red} 35.95 \\
%		butterfly   & $\times$3 & 24.04     & 24.57              & \color{blue} 27.58 & 24.81              & \color{red} 29.11 \\
%		head        & $\times$3 & 32.88     & 33.06              & \color{blue} 33.55 & 32.43              & \color{red} 33.80 \\
%		woman       & $\times$3 & 28.56     & 28.90              & \color{blue} 30.92 & 27.75              & \color{red} 31.87 \\
%		\hline
%		\hline
%		\bf Average & $\times$3 & 30.39     & 30.73              & \color{blue} 32.39 & 29.86              & \color{red} 33.20 \\
%		\hline
%		\hline
%		baby        & $\times$4 & 31.78     & 31.78              & 32.39              & \color{blue} 32.98 & \color{red} 33.11 \\
%		bird        & $\times$4 & 30.18     & 30.25              & 31.12              & \color{blue} 31.98 & \color{red} 32.92 \\
%		butterfly   & $\times$4 & 22.10     & 22.15              & 23.24              & \color{blue} 25.07 & \color{red} 26.45 \\
%		head        & $\times$4 & 31.59     & 31.63              & 31.95              & \color{blue} 32.19 & \color{red} 32.57 \\
%		woman       & $\times$4 & 26.46     & 26.47              & 27.38              & \color{blue} 28.21 & \color{red} 29.33 \\
%		\hline
%		\hline
%		\bf Average & $\times$4 & 28.42     & 28.45              & 29.22              & \color{blue} 30.09 & \color{red} 30.88 \\
%		\hline
%	\end{tabular}
%	\vspace{1pt}
%	\caption{PSNR for scale factor $\times$2 for Set1. {\color{red}Red color} indicates the best performance and {\color{blue}{blue color}} indicates the second best one. Remark that bicubic interpolation works better }
%	\label{tab:SRCNN_Factor_Test}
%\end{table*}
%
%Even with a decent initialization method, training deep models is practically very hard as back-propagated gradients can easily explode as layer-wise gradients are multiplied many times \cite{bengio1994learning}. A popular strategy to avoid explosion is to use very small learning rates.  In SRCNN \cite{Dong2014}, Dong et al. uses the learning rate of $10^{-4}$ for the first two layers and $10^{-5}$ for the last layer. For very deep models, using small learning rates takes more than several months to converge. Instead, we use high learning rates with a gradient clipping strategy proposed to train recurrent neural networks \cite{pascanu2013difficulty}. By bounding gradients, we successfully train a very deep network with learning rate as large as 0.1.  

%
%
%\begin{table*}
%	\small
%	\centering
%	\caption{PSNR for scale factor $\times$4 for Set5. {\color{red}Red color} indicates the best performance and {\color{blue}{blue color}} indicates the second best one. [TODO which methods to benchmark?]}
%	\begin{tabular}
%		{|c|c|c|c|c|c|c|c|c||c|}
%		\hline
%		Set5        & Scale     & {Bicubic} & {Zeyde et al.} & {GR}  & {ANR} & {A+}               & {SRCNN} & {SFSR}             & {SFSR 291}        \\
%		\hline
%		baby        & $\times$2 & 37.07     & -              & 38.32 & 38.44 & 38.52              & 38.30   & \color{blue} 38.55 & \color{red} 38.65 \\
%		bird        & $\times$2 & 36.81     & -              & 38.98 & 40.04 & 41.12              & 40.64   & \color{blue} 41.51 & \color{red} 42.21 \\
%		butterfly   & $\times$2 & 27.43     & -              & 29.06 & 30.48 & 32.01              & 32.20   & \color{blue} 33.31 & \color{red} 34.22 \\
%		head        & $\times$2 & 34.86     & -              & 35.60 & 35.66 & 35.77              & 35.64   & \color{blue} 35.77 & \color{red} 35.89 \\
%		woman       & $\times$2 & 32.14     & -              & 33.70 & 34.55 & 35.31              & 34.94   & \color{blue} 35.52 & \color{red} 35.90 \\
%		\hline
%		\hline
%		\bf Average & $\times$2 & 33.66     & -              & 35.13 & 35.83 & 36.55              & 36.34   & \color{blue} 36.93 & \color{red} 37.38 \\
%		\hline
%		\hline
%		baby        & $\times$3 & 33.91     & 35.08          & 34.93 & 35.13 & 35.21              & 35.01   & \color{blue} 35.27 & \color{red} 35.30 \\
%		bird        & $\times$3 & 32.58     & 34.57          & 33.90 & 34.60 & 35.54              & 34.91   & \color{blue} 35.95 & \color{red} 36.22 \\
%		butterfly   & $\times$3 & 24.04     & 25.94          & 25.03 & 25.90 & 27.24              & 27.58   & \color{blue} 29.11 & \color{red} 29.66 \\
%		head        & $\times$3 & 32.88     & 33.56          & 33.48 & 33.63 & 33.77              & 33.55   & \color{blue} 33.80 & \color{red} 33.88 \\
%		woman       & $\times$3 & 28.56     & 30.37          & 29.73 & 30.33 & 31.20              & 30.92   & \color{blue} 31.87 & \color{red} 32.03 \\
%		\hline
%		\hline
%		\bf Average & $\times$3 & 30.39     & 31.90          & 31.41 & 31.92 & 32.59              & 32.39   & \color{blue} 33.20 & \color{red} 33.42 \\
%		\hline
%		\hline
%		baby        & $\times$4 & 31.78     & -              & 32.79 & 33.03 & \color{blue} 33.28 & 32.98   & 33.11              & \color{red} 33.32 \\
%		bird        & $\times$4 & 30.18     & -              & 31.31 & 31.82 & 32.54              & 31.98   & \color{blue} 32.92 & \color{red} 33.21 \\
%		butterfly   & $\times$4 & 22.10     & -              & 23.05 & 23.52 & 24.42              & 25.07   & \color{blue} 26.45 & \color{red} 26.85 \\
%		head        & $\times$4 & 31.59     & -              & 32.10 & 32.27 & 32.52              & 32.19   & \color{blue} 32.57 & \color{red} 32.61 \\
%		woman       & $\times$4 & 26.46     & -              & 27.43 & 27.80 & 28.65              & 28.21   & \color{blue} 29.33 & \color{red} 29.46 \\
%		\hline
%		\hline
%		\bf Average & $\times$4 & 28.42     & -              & 29.34 & 29.69 & 30.28              & 30.09   & \color{blue} 30.88 & \color{red} 31.09 \\
%		\hline
%	\end{tabular}
%\end{table*}